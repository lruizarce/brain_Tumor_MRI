
import streamlit as st
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
import plotly.graph_objects as go
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.metrics import Precision, Recall
import google.generativeai as genai
import PIL.Image
import os
from dotenv import load_dotenv
load_dotenv()
import cv2

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)
output_dir = "saliency_maps"
os.makedirs(output_dir, exist_ok=True)

def generate_explanation(img_path, model_prediction, confidence):
    prompt = f"""You are an expert neurologist. You are tasked with explaining a saliency map of a brain tumor MRI scan. The saliency map was generated by a deep learning model that was trained to classify brain tumors as either glioma, meningioma, pituitary, or no tumor.
    Explain what a saliency map is to the patient in simple terms.
    The deep learning model predicted the image to be of class "{model_prediction}" with a confidence of {confidence * 100}%.
    
    In your response:
    - Explain the regions of the brain the model is focusing on, based on the saliency map. Refer to the regions highlighted in light cyan, those regions where the model is focusing on.
    - Explain possible reasons why the model made the predictions it did.
    - Don't mention anything like "The saliency map highlights the regions the model is focusing on, which are in light cyan" in your explanation.
    - Keep your explanation to 6 sentences.
    """
    img = PIL.Image.open(img_path)
    model = genai.GenerativeModel(model_name="gemini-1.5-flash")
    response = model.generate_content([prompt, img])
    
    return response.text
    
    
def generate_saliency_map(model, img_array, class_index, img_size):
    # Generate saliency map for the given image and model
    with tf.GradientTape() as tape:
        img_tensor = tf.convert_to_tensor(img_array)
        tape.watch(img_tensor)
        predictions = model(img_tensor)
        target_class = predictions[:, class_index]
        
    gradients = tape.gradient(target_class, img_tensor)
    gradients = tf.math.abs(gradients)
    gradients = tf.reduce_max(gradients, axis=-1)
    gradients = gradients.numpy().squeeze()
    
    gradients = cv2.resize(gradients, img_size)
    
    center = (gradients.shape[0] // 2, gradients.shape[1] // 2)
    radius = min(center[0], center[1]) - 10
    y, x = np.ogrid[:gradients.shape[0], :gradients.shape[1]]
    mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2
    
    gradients = gradients * mask
    brain_gradients = gradients[mask]
    if brain_gradients.max() > brain_gradients.min():
        brain_gradients = (brain_gradients - brain_gradients.min()) / (brain_gradients.max() - brain_gradients.min())
    gradients[mask] = brain_gradients
    
    threshold = np.percentile(gradients[mask], 80)
    gradients[gradients < threshold] = 0
    
    gradients = cv2.GaussianBlur(gradients, (11, 11), 0)
    heatmap = cv2.applyColorMap(np.uint8(255 * gradients), cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
    
    heatmap = cv2.resize(heatmap, img_size)
    
    original_img = image.img_to_array(img)
    superimposed_img = heatmap * 0.7 + original_img * 0.3
    superimposed_img = superimposed_img.astype(np.uint8)
    
    img_path = os.path.join(output_dir, uploaded_file.name)
    with open(img_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
        
    saliency_map_path = f"saliency_maps/{uploaded_file.name}"
    
    cv2.imwrite(saliency_map_path, cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR))
    
    return superimposed_img

def load_xception_model(model_path):
    # Define the shape of the input images
    img_shape = (299, 299, 3)

    # Load the Xception model pre-trained on ImageNet, excluding the top layer
    base_model = tf.keras.applications.Xception(include_top=False,
                                                weights="imagenet",
                                                input_shape=img_shape,
                                                pooling='max')

    # Create a Sequential model
    model = Sequential([
        base_model,  # Add the base model
        Flatten(),  # Flatten the output of the base model
        Dropout(rate=0.3),  # Add a dropout layer with a rate of 0.3
        Dense(128, activation='relu'),  # Add a dense layer with 128 units and ReLU activation
        Dropout(rate=0.25),  # Add another dropout layer with a rate of 0.25
        Dense(4, activation='softmax')  # Add a dense layer with 4 units and softmax activation for classification
    ])
    model.compile(Adamax(learning_rate=0.001), loss="categorical_crossentropy",
                  metrics=["accuracy", Precision(), Recall()])
    model.load_weights(model_path)
    
    return model

# Streamlit app title and description
st.title("Brain Tumor Classification")
st.write("Upload an image of a brain MRI scan to classify")
uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file:
    # Model selection
    selected_model = st.radio(
        "Select Model",
        ("Transfer Learning - Xception", "Custom CNN")
    )
    
    if selected_model == "Transfer Learning - Xception":
        model = load_xception_model("../weights/xception_model.weights.h5")
        img_size = (299, 299)
    else:
        model = load_model("../weights/custom_model.weights.h5")
        img_size = (224, 224)
        
    labels = ["Glioma", "Meningioma", "No tumor", "Pituitary"]
    img = image.load_img(uploaded_file, target_size=img_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0
    
    prediction = model.predict(img_array)
    
    class_index = np.argmax(prediction[0])
    result = labels[class_index]
    
    # Display prediction results
    st.write(f"Predicted Class: {result}")
    st.write(f"Predictions:")
    for label, prob in zip(labels, prediction[0]):
        st.write(f"{label}: {prob:.4f}")
        
    # Generate and display saliency map
    saliency_map = generate_saliency_map(model, img_array, class_index, img_size)
    
    col1, col2 = st.columns(2)
    with col1:
        st.image(uploaded_file, caption="Uploaded Image", use_container_width=True)
    with col2:
        st.image(uploaded_file, caption="Saliency Map", use_container_width=True)
        
        
    saliency_map_path = f"saliency_maps/{uploaded_file.name}"
    explanation = generate_explanation(saliency_map_path,result, prediction[0][class_index])
    
    st.write("## Explanation")
    st.write(explanation)
